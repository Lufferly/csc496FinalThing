{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c4f081f",
   "metadata": {},
   "source": [
    "First, we must construct our environment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ba1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from rl_glue import RLGlue\n",
    "from Agent import BaseAgent \n",
    "from Environment import BaseEnvironment  \n",
    "from manager import Manager\n",
    "import torch\n",
    "\n",
    "import jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4463da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like shown previously, lets declare first, then implement\n",
    "class CaptureGoEnvironment(BaseEnvironment):\n",
    "    def env_init(self, env_info={}):\n",
    "        return NotImplementedError\n",
    "    \n",
    "    def env_start(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_step(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def env_cleanup(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Gets the value of a position on the game-board\n",
    "    def board_index(self, x, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def inBounds(self, x, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_liberties(self, x, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def get_liberties_helper(self, x, y, group_color, visted_coords):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def did_group_captured(self, x, y):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Helper: Print the board\n",
    "    def print_go_board(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Helper: Get the size of the board (how many tiles)\n",
    "    def go_board_size(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Helper: get the board (state)\n",
    "    def get_board(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Helper: get the board state in the form of a tensor\n",
    "    def get_board_tensor(self): \n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Turns an action state into an x y pair\n",
    "    # This is a litttllleee redundant since we do a bunch of transforms, but it is easier for me to read\n",
    "    def action_int_to_xy(self, action_int):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2c82c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CaptureGoEnvironment\n",
    "\n",
    "# Create the environment, declare its variables, etc\n",
    "def env_init(self, env_info={}):\n",
    "\n",
    "    # Declare our variables\n",
    "    reward = None\n",
    "\n",
    "    # The board is an array of height x width\n",
    "    # If (x,y) on the board is 0, no stone\n",
    "    #   if its 1, then a white stone is there\n",
    "    #   if its -1, then a black stone is there\n",
    "    self.board = None\n",
    "    \n",
    "    termination = None\n",
    "\n",
    "    # Which player's turn is it?\n",
    "    # 0 = white's turn\n",
    "    # 1 = black's turn\n",
    "    self.turn = None\n",
    "\n",
    "    # Who won?\n",
    "    # 0 = nobody yet\n",
    "    # 1 = white\n",
    "    # -1 = black\n",
    "    self.who_won = None\n",
    "\n",
    "    self.reward_board_termination = (reward, self.board, termination)\n",
    "\n",
    "    # Set the default board height and width\n",
    "    self.board_height = env_info.get(\"board_height\", 5) \n",
    "    self.board_width = env_info.get(\"board_width\", 5)\n",
    "    # Set the default reward per step\n",
    "    self.reward_per_step = env_info.get(\"reward_per_step\", -1)\n",
    "    # Set the default winning reward and losing reward\n",
    "    self.winning_reward = env_info.get(\"winning_reward\", 100)\n",
    "    self.losing_reward = env_info.get(\"losing_reward\", -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6bfd10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CaptureGoEnvironment\n",
    "\n",
    "# Initialize the environment, set its variables, etc\n",
    "# Called before an agent does anything\n",
    "# Returns the initialized (full of zeros) board\n",
    "def env_start(self):\n",
    "\n",
    "    # Actually initalize\n",
    "    reward = 0\n",
    "    # Initialize the board, an array of 0 ints\n",
    "    self.board = np.zeros(self.board_height * self.board_width, dtype=np.int8)\n",
    "    termination = False\n",
    "    \n",
    "    self.reward_board_termination = (reward, self.board, termination)\n",
    "\n",
    "    # We start as white's turn\n",
    "    self.turn = 0\n",
    "    # Nobody won yet\n",
    "    self.who_won = 0\n",
    "\n",
    "    return self.reward_board_termination[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "562d861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CaptureGoEnvironment\n",
    "\n",
    "# Helper: Print the board\n",
    "def print_go_board(self):\n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    for x in range(self.board_width):\n",
    "        for y in range(self.board_height):\n",
    "            pos = self.board[self.board_index(x, y)]\n",
    "\n",
    "            stone = \".\"\n",
    "            if pos == 1:\n",
    "                stone = \"w\"\n",
    "            if pos == -1:\n",
    "                stone = \"b\"\n",
    "\n",
    "            print(stone, end=\"\")\n",
    "\n",
    "        print(\"\") # print a newline\n",
    "\n",
    "# Helper: Get the size of the go board\n",
    "def go_board_size(self):\n",
    "    return len(self.board)\n",
    "\n",
    "# Helper: Get the board (state)\n",
    "def get_board(self):\n",
    "    return self.board\n",
    "\n",
    "# Helper: get the board state in the form of a tensor, also transforms into a float32 since thats what nn.linear expects\n",
    "#   maybe in the future make it so the nn.Linear takes int8, but for now this should work\n",
    "def get_board_tensor(self):\n",
    "    return torch.from_numpy(self.board).to(torch.float32)\n",
    "\n",
    "# Turns an action state into an x y pair\n",
    "# This is a litttllleee redundant since we do a bunch of transforms, but it is easier for me to read\n",
    "def action_int_to_xy(self, action_int):\n",
    "    y = action_int // self.board_height\n",
    "    x = action_int % self.board_width\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2b7d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "testenv = env = CaptureGoEnvironment()\n",
    "testenv.env_init({\"board_height\" : 5, \"board_width\" : 5 })\n",
    "testenv.env_start()\n",
    "\n",
    "print(env.action_int_to_xy(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d557afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CaptureGoEnvironment\n",
    "## Gets the index of a position on the game-board, given an x y coordinate\n",
    "#   Returns the index where that position on the board is found\n",
    "#   top right is x = 0 y = 0  \n",
    "\n",
    "#   On the actual board array:\n",
    "#       0 means that position is empty\n",
    "#       1 means there is a white stone\n",
    "#       -1 means there is a black stone\n",
    "def board_index(self, x, y):\n",
    "    return (self.board_width * y) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83902d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "env = CaptureGoEnvironment()\n",
    "env.env_init({\"board_height\" : 5, \"board_width\" : 5 })\n",
    "print(env.env_start())\n",
    "print(env.board_index(3, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720c33aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CaptureGoEnvironment\n",
    "\n",
    "# Returns true if the coordinate is in the board's bounds, false otherwise\n",
    "def inBounds(self, x, y):\n",
    "    if ((x < 0) or (y < 0)):\n",
    "        return False\n",
    "\n",
    "    if ((x >= self.board_width) or (y >= self.board_height)):\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Recursively get the liberties of a group of stones\n",
    "#   returns the amount of liberties a group has, and all the coordinates we have visited\n",
    "#   if group_color == 1, then we are looking for a white group\n",
    "#   if group_color == -1, then we are looking for a black group\n",
    "# visited_coords is a list of all coordinates we have visted, so we dont enter into an endless loop\n",
    "def get_liberties_helper(self, x, y, group_color, visited_coords):\n",
    "    if (x, y) in visited_coords:\n",
    "        return 0, visited_coords\n",
    "    visited_coords.append((x, y))\n",
    "\n",
    "    # Check our bounds\n",
    "    if not self.inBounds(x, y):\n",
    "        return 0, visited_coords\n",
    "\n",
    "    # Check what stone (or lack thereof) is on this position\n",
    "    if self.board[self.board_index(x, y)] == 0:\n",
    "        # liberty!\n",
    "        return 1, visited_coords\n",
    "    if self.board[self.board_index(x, y)] != group_color:\n",
    "        # Blocked liberty\n",
    "        return 0, visited_coords\n",
    "    # otherwise this is part of the group, check the neighbors\n",
    "    liberties = 0\n",
    "    up, visited_coords = self.get_liberties_helper(x, y + 1, group_color, visited_coords)\n",
    "    down, visited_coords = self.get_liberties_helper(x, y - 1, group_color, visited_coords)\n",
    "    right, visited_coords = self.get_liberties_helper(x + 1, y, group_color, visited_coords)\n",
    "    left, visited_coords = self.get_liberties_helper(x - 1, y, group_color, visited_coords)\n",
    "\n",
    "    liberties = up + down + left + right\n",
    "    return liberties, visited_coords\n",
    "\n",
    "# Get the liberties of a group, starting at x, y\n",
    "def get_liberties(self, x, y):\n",
    "    # Set the group_color to be whatever color we started at\n",
    "    liberties = 0\n",
    "    coords = []\n",
    "    liberties, coords = self.get_liberties_helper(x, y, self.board[self.board_index(x, y)], visited_coords=[])\n",
    "    return liberties\n",
    "\n",
    "\n",
    "# Returns if any group got captured, given the most recent move\n",
    "#   returns 0 if no groups got captured\n",
    "#   returns 1 if a white group got captured\n",
    "#   returns -1 if a black group got captured\n",
    "#\n",
    "#   Parameters\n",
    "#   x, y : The most recent move\n",
    "def did_group_captured(self, x, y):\n",
    "    # The (x, y) offsets we will check\n",
    "    check_offsets_list = [\n",
    "        [0, 0], \n",
    "        [0, 1], \n",
    "        [0, -1], \n",
    "        [1, 0], \n",
    "        [-1, 0]]\n",
    "\n",
    "    for offset in check_offsets_list:\n",
    "        this_x = x + offset[0]\n",
    "        this_y = y + offset[1]\n",
    "\n",
    "        if not self.inBounds(this_x, this_y):\n",
    "            # This offset is not on the board\n",
    "            continue\n",
    "\n",
    "        # TODO: Make this respect who placed most recently, so that if you capture yourself and another stone the other stone gets captured first\n",
    "        liberties_found = self.get_liberties(this_x, this_y)\n",
    "        if (liberties_found == 0):\n",
    "            # A group got captured, return 1 if it was a white group, -1 if it was a black group\n",
    "            return self.board[self.board_index(this_x, this_y)]\n",
    "\n",
    "\n",
    "    return 0 # no captures\n",
    "\n",
    "# Take a step in the environment, actually place a stone on the board\n",
    "#   Attempt to place a stone at the x, y coordinates\n",
    "#   if player == self.turn, then we actually do this move, otherwise its not that players turn so we do nothing\n",
    "#   if a player attempts to play an invalid move, such as out of bounds or ontop of a stone, we just ignore it and dont change the turn\n",
    "# \n",
    "#   Returns the reward_board_termination tuple that represents the changed environment\n",
    "def env_step(self, x, y, player):\n",
    "    # Check if the game has already finished or not\n",
    "    if (self.who_won != 0):\n",
    "        return None\n",
    "\n",
    "    if (player != self.turn):\n",
    "        # Its not this player's turn\n",
    "        reward = 0\n",
    "        term = self.reward_board_termination[2]\n",
    "        self.reward_board_termination = (reward, self.board, term)\n",
    "        return self.reward_board_termination\n",
    "\n",
    "    # Check that the move is in bounds\n",
    "    if not self.inBounds(x, y):\n",
    "        # Just pretend like this turn didn't happen, but take away some points\n",
    "        reward = -5 # you aren't following the rules!\n",
    "        term = self.reward_board_termination[2]\n",
    "        self.reward_board_termination = (reward, self.board, term)\n",
    "        return self.reward_board_termination\n",
    "\n",
    "    # Check that the position is empty\n",
    "    if (self.board[self.board_index(x, y)] != 0):\n",
    "        # Just pretend like this turn didn't happen, but take away some points\n",
    "        reward = -5 # you aren't following the rules!\n",
    "        term = self.reward_board_termination[2]\n",
    "        self.reward_board_termination = (reward, self.board, term)\n",
    "        return self.reward_board_termination\n",
    "\n",
    "    # If it's white's turn, stone = 1\n",
    "    #   if it's black's turn, stone = -1\n",
    "    stone = 0\n",
    "    if (self.turn == 0):\n",
    "        stone = 1\n",
    "    else:\n",
    "        stone = -1\n",
    "\n",
    "    # Place the stone on the board\n",
    "    self.board[self.board_index(x, y)] = stone\n",
    "\n",
    "\n",
    "    # Get the reward\n",
    "    reward = self.reward_per_step\n",
    "\n",
    "    # Most complicated part: check if anything was captured, and therefore if we need to terminate or not\n",
    "    termination = False\n",
    "    captured_group = self.did_group_captured(x, y)\n",
    "    if (captured_group == 1):\n",
    "        # white got captured\n",
    "        if (self.turn == 1): # If it is white's turn right now, we are giving white the reward\n",
    "            reward += self.losing_reward\n",
    "        else:\n",
    "            reward += self.winning_reward\n",
    "        \n",
    "        termination = True\n",
    "    elif (captured_group == -1):\n",
    "        # black got captured\n",
    "        if (self.turn == 1): # If it is white's turn right now, we are giving white the reward\n",
    "            reward += self.winning_reward\n",
    "        else:\n",
    "            reward += self.losing_reward\n",
    "\n",
    "        termination = True\n",
    "\n",
    "    \n",
    "    # The winner was who did not get captured, or nobody if nothing was captured\n",
    "    self.who_won = captured_group * -1\n",
    "    # It is now the other player's turn\n",
    "    self.turn = not self.turn\n",
    "    self.reward_board_termination = (reward, self.board, termination)\n",
    "\n",
    "    return self.reward_board_termination\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d73de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "(-101, array([-1,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,\n",
      "        0,  0, -1,  0,  0,  0, -1,  1], dtype=int8), True)\n",
      "b....\n",
      ".w...\n",
      ".....\n",
      "..w.b\n",
      "...bw\n",
      "\n",
      "=============\n",
      "\n",
      ".....\n",
      ".....\n",
      ".wbw.\n",
      ".wbww\n",
      ".wbbb\n",
      "(-101, [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, -1, -1, -1, 0, 0, 1, 1, -1, 0, 0, 0, 1, -1], True)\n",
      ".....\n",
      "..w..\n",
      ".wbw.\n",
      ".wbww\n",
      ".wbbb\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "# Testing\n",
    "env = CaptureGoEnvironment()\n",
    "env.env_init({\"board_height\" : 5, \"board_width\" : 5 })\n",
    "print(env.env_start())\n",
    "\n",
    "white = 0\n",
    "black = 1\n",
    "env.env_step(3, 2, white)\n",
    "env.env_step(3, 4, black)\n",
    "env.env_step(1, 1, white)\n",
    "env.env_step(0, 0, black)\n",
    "env.env_step(4, 4, white)\n",
    "print(env.env_step(4, 3, black))\n",
    "\n",
    "env.print_go_board()\n",
    "\n",
    "print(\"\\n=============\\n\")\n",
    "env = CaptureGoEnvironment()\n",
    "env.env_init({\"board_height\" : 5, \"board_width\" : 5 })\n",
    "env.env_start()\n",
    "\n",
    "env.board = [0, 0, 0, 0, 0, \n",
    "             0, 0,  1,  1,   1,\n",
    "             0, 0, -1, -1,  -1,\n",
    "             0, 0,  1,  1,  -1,\n",
    "             0, 0,  0,  1,  -1,]\n",
    "env.print_go_board()\n",
    "print(env.env_step(1, 2, white))\n",
    "env.print_go_board()\n",
    "print(env.go_board_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d08dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to CaptureGoEnvironment\n",
    "\n",
    "def env_cleanup(self):\n",
    "    reward = 0\n",
    "    # Initialize the board, an array of 0 ints\n",
    "    self.board = np.zeros(self.board_height * self.board_width, dtype=np.int8)\n",
    "    termination = False\n",
    "    \n",
    "    self.reward_board_termination = (reward, self.board, termination)\n",
    "\n",
    "    # We start as white's turn\n",
    "    self.turn = 0\n",
    "    # Nobody won yet\n",
    "    self.who_won = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d2939",
   "metadata": {},
   "source": [
    "Now, lets implement an agent that can learn\n",
    "I used the pytorch deep-q learning tutorial to help me with this https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "Deep-q learning is how google created it's super strong go neural net, so I think I might have some success\n",
    "\n",
    "We say that the action an agent can take is placing a stone on a tile, so we have as many actions as tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ee4439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc3ce97",
   "metadata": {},
   "source": [
    "Create the replay memory, used https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html to help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e1efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our replay memory for the deep-q neuralNet\n",
    "class DeepQReplayMemory(object):\n",
    "    # Create the replay memory object, capacity is how much memory we store\n",
    "    # The memory replay object is essentially just a queue that stores memory\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    # Push a single observation into the queue\n",
    "    def push(self, state, action, state_prime, reward):\n",
    "        self.memory.append((state, action, state_prime, reward)) # append this observation to our memory\n",
    "\n",
    "    # Sample a random observation from the replay memory of sample_size\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "    \n",
    "    # Get how much memory we have\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f9740f",
   "metadata": {},
   "source": [
    "Create the actual deep-q neural network, mostly relies on pytorch to do the dirty work.\n",
    "Again, following the deep-q tutorial for pytorch https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "003e7997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The deep-q network for our go game\n",
    "class goDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, hidden_layer_1_size, hidden_layer_2_size, num_outputs):\n",
    "        super(goDQN, self).__init__()\n",
    "\n",
    "        # Create the layers, I actually want multiple hidden layers\n",
    "        self.input_layer = nn.Linear(num_inputs, hidden_layer_1_size) # Create the input layer\n",
    "        self.hidden_layer_1 = nn.Linear(hidden_layer_1_size, hidden_layer_2_size) # Create the first hidden layer\n",
    "        self.hidden_layer_2 = nn.Linear(hidden_layer_2_size, hidden_layer_2_size) # Create the second hidden layer\n",
    "        self.output_layer = nn.Linear(hidden_layer_2_size, num_outputs)\n",
    "    \n",
    "    # Process an input through the neural net, return what we got as our output\n",
    "    # Returns a tensor representing the output layer's output. A tensor is essentially just a multidimensional matrix with a single datatype\n",
    "    def forward(self, input):\n",
    "        process = input\n",
    "        # Run the input through our layers\n",
    "        process = functional.relu(self.input_layer(process)) # We call relu to make sure we dont pass a negative number into future layers\n",
    "        process = functional.relu(self.hidden_layer_1(process))\n",
    "        process = functional.relu(self.hidden_layer_2(process))\n",
    "        # Return our output\n",
    "        return self.output_layer(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eb076f",
   "metadata": {},
   "source": [
    "Create the training functions for our go agents, we have a black agent and a white agent for each player of go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3db2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_1_SIZE = 128\n",
    "HIDDEN_LAYER_2_SIZE = 64\n",
    "REPLAY_MEMORY_SIZE = 1000\n",
    "\n",
    "# Our epsilon for epsilon_greedy\n",
    "EPSILON = 0.1\n",
    "# Our stepsize / learning rate \n",
    "STEP_SIZE = 0.01\n",
    "# Our discount factor\n",
    "GAMMA = 0.9\n",
    "\n",
    "goEnv = CaptureGoEnvironment()\n",
    "goEnv.env_init({\"board_height\" : 5, \"board_width\" : 5 })\n",
    "goEnv.env_start() # So that the board is actually created and we can grab its size\n",
    "\n",
    "white_agent = goDQN(goEnv.go_board_size(), HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, goEnv.go_board_size())\n",
    "black_agent = goDQN(goEnv.go_board_size(), HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, goEnv.go_board_size())\n",
    "\n",
    "# Create the target networks for each of our agents\n",
    "white_agent_target_network = goDQN(goEnv.go_board_size(), HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, goEnv.go_board_size())\n",
    "black_agent_target_network = goDQN(goEnv.go_board_size(), HIDDEN_LAYER_1_SIZE, HIDDEN_LAYER_2_SIZE, goEnv.go_board_size())\n",
    "\n",
    "white_memory = DeepQReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "black_memory = DeepQReplayMemory(REPLAY_MEMORY_SIZE)\n",
    "\n",
    "# Create our pytorch optimizer, we have to choose an optimizer, it might be worth trying multiple optimizers in the future\n",
    "#   for now we will go with what the tutorial https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "#   says to use, AdamW\n",
    "white_optimizer = optim.AdamW(white_agent.parameters(), lr=STEP_SIZE)\n",
    "black_optimizer = optim.AdamW(black_agent.parameters(), lr=STEP_SIZE)\n",
    "\n",
    "\n",
    "def select_action(agent, board_state):\n",
    "    # Follow epsilon to choose a random state or consult our learned Q\n",
    "    rand_value = random.random() # Get a random number between 0-1\n",
    "    if (rand_value < EPSILON):\n",
    "        # Choose a random action\n",
    "        return torch.tensor([[random.randint(0, goEnv.go_board_size() - 1)]])\n",
    "    else:\n",
    "        # Return the output node with the highest signal, the index of that is our action\n",
    "        #   ie what space we will attempt to place a piece on\n",
    "        #print(board_state)\n",
    "        #print(agent(board_state))\n",
    "\n",
    "        # Get the output with the highest signal as the action to take\n",
    "        agent_outputs = agent(board_state)\n",
    "        max_value = agent_outputs.max()\n",
    "        for i in range(len(agent_outputs)):\n",
    "            if agent_outputs[i] == max_value:\n",
    "                return torch.tensor([[i]])\n",
    "        raise IndexError # For some reason we could not find it?\n",
    "        #return agent(board_state).max().indices.view(1, 1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fb437c",
   "metadata": {},
   "source": [
    "Actually train our agents, again using the pytorch deep-q network tutorial https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "Also used https://www.geeksforgeeks.org/deep-learning/implementing-deep-q-learning-using-tensorflow/ as a tutorial for implementing this\n",
    "\n",
    "This is a deep-q network, so we have a target network and our actual agent networks \n",
    "The target network is only updated every 1000-10000 steps, so that we maintain stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "237199d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "\n",
    "# This part heavily relies on the pytorch tutorial for deep-q learning\n",
    "# https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "def optimize_model(agent_policy_network, agent_target_network, agent_replay_memory, agent_optimizer):\n",
    "    if len(agent_replay_memory < BATCH_SIZE):\n",
    "        return # we dont have enough replay memory to actually create a full batch\n",
    "\n",
    "    # This next bit is copied directly from the tutorial, its a complicated part that turns the replay memory into\n",
    "    # a batch that we can actually use to learn with\n",
    "    transitions = agent_replay_memory.sample(BATCH_SIZE)\n",
    "    states, actions, states_prime, rewards = zip(*transitions) # got from https://www.geeksforgeeks.org/deep-learning/implementing-deep-q-learning-using-tensorflow/\n",
    "\n",
    "    state_batch = torch.cat(states)\n",
    "    action_batch = torch.cat(actions)\n",
    "    reward_batch = torch.cat(actions)\n",
    "\n",
    "    # Get our Q-value for each state, action pair\n",
    "    state_action_Q_values = agent_policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute the expected value for each state_prime\n",
    "    state_prime_expected_values = torch.zeros(BATCH_SIZE) # Initialize theh array\n",
    "    i = 0 # Counter for which element in the batches we are in\n",
    "    for state_prime in states_prime:\n",
    "        this_expected_value = agent_target_network(state_prime).max(1) # Get the output from the TARGET network\n",
    "        this_expected_value = (this_expected_value * GAMMA) + reward_batch[i]\n",
    "\n",
    "        state_prime_expected_values[i] = this_expected_value\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "    # Compute loss, unlike the tutorial I would rather use MSE loss since that is what we used in class\n",
    "    criterion = nn.MSELoss()\n",
    "    losses = criterion(state_action_Q_values, state_prime_expected_values.unsqueeze(1)) # Unsqueeze(1) makes each tensor element a seperate array in a tenso\n",
    "\n",
    "    # optimize our network, ie update the parameters of our policy's network\n",
    "    agent_optimizer.zero_grad() # This resets the gradients of our optimizer\n",
    "    losses.backward() # God function which will actually derive our parameter's sums and\n",
    "    # Here the tutorial clips the gradient values, Im not gonna do that for now to see what happens\n",
    "    agent_optimizer.step() # Actually update the parameters!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab91425f",
   "metadata": {},
   "source": [
    "Next step: Actually implementing the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d2b2748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....\n",
      ".....\n",
      ".w...\n",
      ".....\n",
      ".....\n",
      "None\n",
      "tensor([[7]]) tensor([[2]]) tensor([[1]]) (-1, array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0], dtype=int8), False)\n",
      ".....\n",
      ".....\n",
      ".w...\n",
      "...b.\n",
      ".....\n",
      "None\n",
      "tensor([[18]]) tensor([[3]]) tensor([[3]]) (-1, array([ 0,  0,  0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0, -1,  0,  0,  0,  0,  0,  0], dtype=int8), False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Act on this action\u001b[39;00m\n\u001b[32m     25\u001b[39m this_x, this_y = goEnv.action_int_to_xy(action)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m reward_state_term = \u001b[43mgoEnv\u001b[49m\u001b[43m.\u001b[49m\u001b[43menv_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoEnv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mturn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(goEnv.print_go_board())\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(action, this_x, this_y, reward_state_term)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:116\u001b[39m, in \u001b[36menv_step\u001b[39m\u001b[34m(self, x, y, player)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "# How many episodes would we like to run?\n",
    "NUM_EPISODES = 10\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    # Prepare the environment\n",
    "    goEnv.env_cleanup()\n",
    "    goEnv.env_start()\n",
    "\n",
    "    while (goEnv.who_won == 0): # While the game is still going, and nobody won\n",
    "        # Swap who runs each time, since we have two different agents playing against eachother\n",
    "        current_agent = white_agent\n",
    "        current_agent_target_network = white_agent_target_network\n",
    "        current_agent_replay_memory = white_memory\n",
    "        current_agent_optimizer = white_optimizer\n",
    "        if (goEnv.turn == 1): # its actually black's turn\n",
    "            current_agent = black_agent\n",
    "            current_agent_target_network = black_agent_target_network\n",
    "            current_agent_replay_memory = black_memory\n",
    "            current_agent_optimizer = black_optimizer\n",
    "\n",
    "        # select our action\n",
    "        action = select_action(current_agent, goEnv.get_board_tensor())\n",
    "        \n",
    "        # Act on this action\n",
    "        this_x, this_y = goEnv.action_int_to_xy(action)\n",
    "        reward_state_term = goEnv.env_step(this_x, this_y, goEnv.turn)\n",
    "        print(goEnv.print_go_board())\n",
    "        print(action, this_x, this_y, reward_state_term)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
